<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Universal Approximation Theorem | An Nguyen</title><meta name=keywords content="Generalisation Theory"><meta name=description content="When I first learnt about Deep Learning, I was always wondering how can these neural networks can perform a wide range of complex tasks. The idea of defining an objective function and then train the neural networks to minimise that seems intuitive to understand about the mechanism but does not justify why it would work. It turns out that when I came across the Universal Approximation Theorem, things get clearer."><meta name=author content><link rel=canonical href=/posts/blog1_uat/><link crossorigin=anonymous href=/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=apple-touch-icon href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=/posts/blog1_uat/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})'></script><meta property="og:url" content="/posts/blog1_uat/"><meta property="og:site_name" content="An Nguyen"><meta property="og:title" content="Universal Approximation Theorem"><meta property="og:description" content="When I first learnt about Deep Learning, I was always wondering how can these neural networks can perform a wide range of complex tasks. The idea of defining an objective function and then train the neural networks to minimise that seems intuitive to understand about the mechanism but does not justify why it would work. It turns out that when I came across the Universal Approximation Theorem, things get clearer."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-02-02T22:13:40+00:00"><meta property="article:modified_time" content="2026-02-02T22:13:40+00:00"><meta property="article:tag" content="Generalisation Theory"><meta name=twitter:card content="summary"><meta name=twitter:title content="Universal Approximation Theorem"><meta name=twitter:description content="When I first learnt about Deep Learning, I was always wondering how can these neural networks can perform a wide range of complex tasks. The idea of defining an objective function and then train the neural networks to minimise that seems intuitive to understand about the mechanism but does not justify why it would work. It turns out that when I came across the Universal Approximation Theorem, things get clearer."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"/posts/"},{"@type":"ListItem","position":2,"name":"Universal Approximation Theorem","item":"/posts/blog1_uat/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Universal Approximation Theorem","name":"Universal Approximation Theorem","description":"When I first learnt about Deep Learning, I was always wondering how can these neural networks can perform a wide range of complex tasks. The idea of defining an objective function and then train the neural networks to minimise that seems intuitive to understand about the mechanism but does not justify why it would work. It turns out that when I came across the Universal Approximation Theorem, things get clearer.\n","keywords":["Generalisation Theory"],"articleBody":"When I first learnt about Deep Learning, I was always wondering how can these neural networks can perform a wide range of complex tasks. The idea of defining an objective function and then train the neural networks to minimise that seems intuitive to understand about the mechanism but does not justify why it would work. It turns out that when I came across the Universal Approximation Theorem, things get clearer.\nTheorem The Universal Approximation Theorem, in lose terms, tells us that a feedforward network with a linear output layer and at least one hidden layer with any “squashing” activation function can approximate any continuous function to any desired degree of accuracy. [1]\nThis has given mathematical justifications that large and deep neural networks can model complex non-linear data found in the real world.\nHowever, it should be noted that the existence of such neural networks does not guarantee the learning algorithm would be able to learn that function.\nExperiment Let’s experiment with a toy example. We will try to construct a simple feedforward neural network to approximate the function:\n$$ f(x)=e^{-x^2} $$\nModel architecture\nLinear → Sigmoid → Hidden (hidden=16) → Linear\nTraining configuration\nSetting Value BATCH_SIZE 32 EPOCHS 200 OPTIMISER ADAM LR 1e-3 The source code could be found in here\nEvaluation After the training, here is the graph to compare between our model approximation and the target function. As we can see, they’re very close to each other!\nThe loss over time could be seen below\nThe training and testing loss to a MSE value of 0.000004, which show similarity between two models\nConclusion The experiment shows that even a very simple neural network can approximate a “complex” continuous function quite well. I hope this post gives you an intuitive sense of the theorem and why neural networks are so powerful. I do not cover the proof here, but there are several interesting proofs available online. I may write about them in a future post. Happy reading!\nReferences [1] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. Chapter 6: Deep Feedforward Networks.\n","wordCount":"348","inLanguage":"en","datePublished":"2026-02-02T22:13:40Z","dateModified":"2026-02-02T22:13:40Z","mainEntityOfPage":{"@type":"WebPage","@id":"/posts/blog1_uat/"},"publisher":{"@type":"Organization","name":"An Nguyen","logo":{"@type":"ImageObject","url":"/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=/ accesskey=h title="An Nguyen (Alt + H)">An Nguyen</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=/posts/ title=Posts><span>Posts</span></a></li><li><a href=/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=/tags/ title=Tags><span>Tags</span></a></li><li><a href=/archives/ title=Archives><span>Archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=/>Home</a>&nbsp;»&nbsp;<a href=/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Universal Approximation Theorem</h1><div class=post-meta><span title='2026-02-02 22:13:40 +0000 UTC'>February 2, 2026</span>&nbsp;·&nbsp;<span>2 min</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#theorem aria-label=Theorem>Theorem</a></li><li><a href=#experiment aria-label=Experiment>Experiment</a></li><li><a href=#evaluation aria-label=Evaluation>Evaluation</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>When I first learnt about Deep Learning, I was always wondering how can these neural networks can perform a wide range of complex tasks. The idea of defining an objective function and then train the neural networks to minimise that seems intuitive to understand about the mechanism but does not justify why it would work. It turns out that when I came across the Universal Approximation Theorem, things get clearer.</p><h2 id=theorem>Theorem<a hidden class=anchor aria-hidden=true href=#theorem>#</a></h2><p>The Universal Approximation Theorem, in lose terms, tells us that a feedforward network with a linear output layer and at least one hidden layer with any &ldquo;squashing&rdquo; activation function can approximate any continuous function to any desired degree of accuracy. [1]</p><p>This has given mathematical justifications that large and deep neural networks can model complex non-linear data found in the real world.</p><p>However, it should be noted that the existence of such neural networks does not guarantee the learning algorithm would be able to learn that function.</p><h2 id=experiment>Experiment<a hidden class=anchor aria-hidden=true href=#experiment>#</a></h2><p>Let&rsquo;s experiment with a toy example. We will try to construct a simple feedforward neural network to approximate the function:</p><p>$$
f(x)=e^{-x^2}
$$</p><p><strong>Model architecture</strong></p><p>Linear → Sigmoid → Hidden (hidden=16) → Linear</p><p><strong>Training configuration</strong></p><table><thead><tr><th>Setting</th><th>Value</th></tr></thead><tbody><tr><td>BATCH_SIZE</td><td>32</td></tr><tr><td>EPOCHS</td><td>200</td></tr><tr><td>OPTIMISER</td><td>ADAM</td></tr><tr><td>LR</td><td>1e-3</td></tr></tbody></table><p>The source code could be found in <a href=https://github.com/DavidNguyen58/An_Blog/blob/main/source_code/blog1_UAT.ipynb>here</a></p><h2 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h2><p>After the training, here is the graph to compare between our model approximation and the target function. As we can see, they&rsquo;re very close to each other!</p><p><img alt="Model approximation vs. target function" loading=lazy src=/uat_img1.png></p><p>The loss over time could be seen below</p><p>The training and testing loss to a MSE value of 0.000004, which show similarity between two models</p><p><img alt="Training and testing loss over epochs" loading=lazy src=/uat_img2.png></p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>The experiment shows that even a very simple neural network can approximate a &ldquo;complex&rdquo; continuous function quite well. I hope this post gives you an intuitive sense of the theorem and why neural networks are so powerful. I do not cover the proof here, but there are several interesting proofs available online. I may write about them in a future post. Happy reading!</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. <em>Deep Learning</em>. MIT Press, 2016. Chapter 6: Deep Feedforward Networks.</p></div><footer class=post-footer><ul class=post-tags><li><a href=/tags/generalisation-theory/>Generalisation Theory</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=/>An Nguyen</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>